{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Random\n",
    "import java.util\n",
    "import org.apache.spark.ml.classification.NaiveBayes\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession, _}\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies}\n",
    "import org.apache.spark.streaming.receiver.Receiver\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n",
    "import org.apache.kafka.common.serialization.StringDeserializer\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.util.{DoubleAccumulator, LongAccumulator}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object CorrectAccumulator {\n",
    "  @volatile private var instance: DoubleAccumulator = null\n",
    "  def getInstance(sc: SparkContext): DoubleAccumulator = {\n",
    "    if (instance == null) {\n",
    "      synchronized {\n",
    "        if (instance == null) {\n",
    "          instance = sc.doubleAccumulator(\"Accuracy\")\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    instance\n",
    "  }\n",
    "}\n",
    "\n",
    "object TrialCounter {\n",
    "  @volatile private var instance: LongAccumulator = null\n",
    "  def getInstance(sc: SparkContext): LongAccumulator = {\n",
    "    if (instance == null) {\n",
    "      synchronized {\n",
    "        if (instance == null) {\n",
    "          instance = sc.longAccumulator(\"Counter\")\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    instance\n",
    "  }\n",
    "}\n",
    "\n",
    "object SpamHamUtils {\n",
    "  val SMSSchema: types.StructType = types.StructType(Array(\n",
    "    types.StructField(\"spamorham\", types.StringType, nullable = true),\n",
    "    types.StructField(\"message\", types.StringType, nullable = true)\n",
    "  ))\n",
    "\n",
    "  def lineToRow(line: RDD[String]): RDD[Row] = {\n",
    "    line.map(_.split(\"\\t\")).map(r => Row(r.head, r.tail.mkString(\" \")))\n",
    "  }\n",
    "\n",
    "  def parseTrainingData(p: String, s: SparkSession): DataFrame = {\n",
    "    val rawRDD = s.sparkContext.textFile(p).map(_.split(\"\\t\")).map { r =>\n",
    "      Row(r.head, r.tail.mkString(\" \"))\n",
    "    }\n",
    "    s.createDataFrame(rawRDD, SMSSchema)\n",
    "  }\n",
    "\n",
    "  def evaluateModel(predictions: DataFrame): Double = {\n",
    "    val evaluator = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"label\")\n",
    "      .setPredictionCol(\"prediction\")\n",
    "      .setMetricName(\"accuracy\")\n",
    "    val accuracy = evaluator.evaluate(predictions)\n",
    "    println(\"Test set accuracy = \" + accuracy)\n",
    "    accuracy\n",
    "  }\n",
    "\n",
    "  // Randomly Sample from an array weighted by how frequently the element occurs\n",
    "  def weightedSample(wordCounts: Array[(String, Int)], sampleCount: Int): Seq[String] = {\n",
    "    def selectInternal(acc: Int): String = {\n",
    "      val shuffled = Random.shuffle[(String, Int), Seq](wordCounts)\n",
    "      var _r = acc\n",
    "      for ((w, c) <- shuffled) {\n",
    "        _r -= c\n",
    "        if (_r <= 0) {\n",
    "          return w\n",
    "        }\n",
    "      }\n",
    "      \"\"\n",
    "    }\n",
    "\n",
    "    val totalWeight = wordCounts.foldLeft(0)((ac, t) => ac + t._2)\n",
    "    (0 until sampleCount).map { i =>\n",
    "      val r = Random.nextInt(totalWeight)\n",
    "      selectInternal(r)\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import the Spam/Ham model from HDFS\n",
    "val modelUri = \"hdfs:///nb_model\"\n",
    "val model = PipelineModel.load(modelUri)\n",
    "\n",
    "// Get messages from Kafka\n",
    "val ssc = new StreamingContext(spark.sparkContext, Seconds(2))\n",
    "val topic = \"top1\"\n",
    "val brokers = \"kafka-0-broker.${APPNAME}proddataserviceskafka.autoip.dcos.thisdcos.directory:1025\"\n",
    "val props = Map[String, Object](\n",
    "  \"bootstrap.servers\" -> brokers,\n",
    "  \"key.deserializer\" -> classOf[StringDeserializer],\n",
    "  \"value.deserializer\" -> classOf[StringDeserializer],\n",
    "  \"group.id\" -> \"consumer1\",\n",
    "  \"auto.offset.reset\" -> \"earliest\",\n",
    "  \"enable.auto.commit\" -> (false: java.lang.Boolean),\n",
    "  \"sasl.kerberos.service.name\" -> \"kafka\",\n",
    "  \"security.protocol\" -> \"SASL_SSL\",\n",
    "  \"sasl.mechanism\" -> \"GSSAPI\",\n",
    "  \"ssl.truststore.location\" -> \"/mnt/mesos/sandbox/secrets/trust-ca.jks\",\n",
    "  \"ssl.truststore.password\" -> \"changeit\"\n",
    ")\n",
    "val messages = KafkaUtils.createDirectStream[String, String](\n",
    "  ssc,\n",
    "  LocationStrategies.PreferConsistent,\n",
    "  ConsumerStrategies.Subscribe[String, String](Array(topic), props))\n",
    "val lines = messages.map(_.value)\n",
    "\n",
    "lines.foreachRDD { rdd: RDD[String] =>\n",
    "    // Parse the messages\n",
    "    val raw = spark.createDataFrame(SpamHamUtils.lineToRow(rdd), SpamHamUtils.SMSSchema)\n",
    "    // Predict Spam/Ham using the model\n",
    "    val predictions = model.transform(raw)\n",
    "    // Calculate the accuracy of the model\n",
    "    val correct = CorrectAccumulator.getInstance(rdd.sparkContext)\n",
    "    val total = TrialCounter.getInstance(rdd.sparkContext)\n",
    "    predictions.foreach { r =>\n",
    "      val label = r.getAs[Double](2)\n",
    "      val prediction = r.getAs[Double](8)\n",
    "      println(prediction)\n",
    "      if (prediction == label) correct.add(1)\n",
    "      total.add(1)\n",
    "    }\n",
    "    println(s\"Running accuracy \${correct.value.toFloat / total.value}\")\n",
    "}\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.kafka.clients.consumer.KafkaConsumer\n",
    "import scala.collection.JavaConverters._\n",
    "\n",
    "val TOPIC=\"top1\"\n",
    "\n",
    "val  props = new java.util.Properties()\n",
    "props.put(\"bootstrap.servers\", \"kafka-0-broker.${APPNAME}proddataserviceskafka.autoip.dcos.thisdcos.directory:1025\")\n",
    "props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\")\n",
    "props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\")\n",
    "props.put(\"group.id\", \"something\")\n",
    "props.put(\"auto.offset.reset\", \"latest\")\n",
    "props.put(\"security.protocol\", \"SASL_SSL\")\n",
    "props.put(\"ssl.truststore.location\", \"/mnt/mesos/sandbox/secrets/trust-ca.jks\")\n",
    "props.put(\"ssl.truststore.password\", \"changeit\")\n",
    "props.put(\"sasl.mechanism\", \"GSSAPI\")\n",
    "props.put(\"sasl.kerberos.service.name\", \"kafka\")\n",
    "\n",
    "val consumer = new KafkaConsumer[String, String](props)\n",
    "\n",
    "consumer.subscribe(java.util.Collections.singletonList(TOPIC))\n",
    "\n",
    "while(true){\n",
    "    val records=consumer.poll(1)\n",
    "    for (record<-records.asScala){\n",
    "        println(record)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
